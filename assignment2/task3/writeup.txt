For starters I run the code form the blog up until to the point where adding a sparsity constraint on the encoded representations. 

The first run took about a minute running on tritanium with epochs set to 50 runs. The results are to be seen below. Afterwards I played around with that number and run it again on 100 epochs the which took around five minutes on the same machine. One can't help but to notice how accurate the model is shaping the images. Finally I bumped that number towards 500 for more accurate results which took exactly 16 minutes [save machine] and used around 542MB of ram. I believe if one keeps running this for several hours would finally reach the same accurate results as in the original image. 

Adding a sparsity constraint on the encoded representations
------------------------------------------------------------
Afterwards I trained the model for 100 epochs with added regularization which reduces the likelihood for the model to over-fit and can be trained longer. It took around ~4 minutes and took the same amount of RAM. The model ends with a  loss of 0.27 on both training and testing. The first-sight results were pretty bad for some unknown reason to me. I tweaked the params for several times, tried different settings, but unfortunately couldnâ€™t get the right results as mentioned in the blog. 

Deep autoencoder
----------------
The next step was to not limit ourselves to asingle layer as encoder or decoder, we could instead use a stack of layers, such as:

[show code]


Convolutional autoencoder
---------------------------
Using convolution neural networks on images makes somehow sense. In practical settings, autoencoders applied to images are always convolutional autoencoders --they simply perform much better. In the blog [1] there is an implementation provided of an encoder which consits of Conv2d and MaxPooling2D layers (max pooling being used for spatial down-sampling), decoder will consist in a stack of Conv2D and UpSampling2D layers.
We ran the implementation on 50 epochs and below is our results. our loss function was about loss: 0.1024 and val loss function 0.1069 which is somehow better than our previous models converges

Below our results on 50 epochs

[show image]

We also tried the solution on the 128-dimensional representations. These, however, are 8x4x4, so we reshaped them to 4x32 in order to be able to display them as grayscale images.

[show image]


Application to image denoising
-----------------------------
Afterwards we experimented by applying image denosing problem to convolutional autoencoder. So basically we train our existing autoencoder to map the noisy digits images to clear digits images. We applied gaussian noise matrix and cliped the images between 0 and 1

Below our results 

[show image]

As you can see the image is barely recognizable. We than compared this approach to our previous convolutional autoencoder, this in order to improve the quality of the reconstructed image.


After slight modification in the code we ran the model for 100 epochs on the training and below is our results. 

[show image] 


Sequence-to-sequence autoencoder
-------------------------------
Let's suppose our inputs are sequences rather than vectors or 2D images. In this case we may want to use as encoder and decorder a type of model that can capture temporal structure, such as LSTM. We experimented with LSTM-based autoencoder, first used a LSTM encoder to turn our inputs sequences into a single vector that contains information about the entire sequence, then repeat this vector n times (where n is the number of timesteps in the output sequence) and ran a LSTM decoder to turn this constant sequence into the target sequence.

[show image]


Variational Autoencoder (VAE)
------------------------------
Variational auto-encoders are a slightly more modern and interesting take on autoencoding.
VAE is a type of auto-encoder with added constraints on the encoded representations being learned. More precisely, it is an autoencoder that learns a latent variable model for its input data. So instead of letting your neural network learn an arbitrary function, you are learning the parameters of a probability distribution modeling your data. If you sample points from this distribution, you can generate new input data samples: a VAE is a "generative model".




[1] https://blog.keras.io/building-autoencoders-in-keras.html

