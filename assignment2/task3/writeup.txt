For starters I run the code form the blog up untill to the point where adding a sparsity constraint on the encoded representations. 
The first run took about a minute running on tritanium with epochs set to 50 runs. The results are to be seen below. Afterwards I played around with that number and run it again on 100 epochs the which took around five minutes on the same machine. One can't help but to notice how accurate the model is shaping the images. Finally I bumped that number towards 500 for more accurate results which took actactly 16 minutes [save machine] and used around 542MB of ram. I believe if one keeps running this for serveral hours would finally reach the same accurate results as in the orginal image. 

Adding a sparsity constraint on the encoded representations
------------------------------------------------------------
Afterwards I trained the model for 100 epochs with added regularization which reduces the likelihood for the model to overfit and can be trained longer. It took around ~4 mintues and took the same amount of RAM. The model ends with a  loss of 0.27 on both training and testing. Below the visualization

[the results are pretty bad!]

At first the results were pretty bad for some unknown reason to me. I run this on the DSLabs and below is the results. 


Deep autoencoder
----------------

We do not have to limit ourselves to asingle layer as encoder or decoder, we could instead use a stack of layers, such as:


